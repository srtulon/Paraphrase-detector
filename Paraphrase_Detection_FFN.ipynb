{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase-Detection using Feature Fusion Network\n",
    "Paraphrase detection is the task of examining two text entities (ex. sentence) and determining whether they have the same meaning. In order to obtain high accuracy on this task, thorough syntactic and semantic analysis of the two text entities is required.\n",
    "\n",
    "## What is Paraphrase?\n",
    "In simple words, paraphrase is just an alternative representation of the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text_similarity.png](text_similarity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Question Pairs Dataset\n",
    "There are over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line truly contains a duplicate pair.\n",
    "\n",
    "We can download dataset from [Quora Question Pairs Dataset](https://www.kaggle.com/quora/question-pairs-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "vWnY8YdR1YMb",
    "outputId": "0ccb4c84-a142-42da-ecb1-796f7206b03f"
   },
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Permute, dot, add, concatenate\n",
    "from tensorflow.keras.layers import Embedding,Input, Dense, Dropout, Reshape, BatchNormalization, TimeDistributed, Lambda, Concatenate,concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, f1_score, log_loss, confusion_matrix\n",
    "import spacy\n",
    "!pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "# import nltk\n",
    "# nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WTv5Folm5llV"
   },
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxaDCDbS2sQD"
   },
   "outputs": [],
   "source": [
    "project_path = 'paraphrase_detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "KkTEILOP1dRn",
    "outputId": "a9a83d47-4126-4958-e56c-c2275eee9b48"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(project_path+\"questions.csv\",nrows=10000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZr-ay9J5q6E"
   },
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSM_jivf3ZTs"
   },
   "outputs": [],
   "source": [
    "# prepare translation table for removing punctuation\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "def clean_question(text):\n",
    "    doc = sp(text)\n",
    "    # tokenize\n",
    "    # text = text.split()\n",
    "    # Lemmatization\n",
    "    text = [token.lemma_ for token in doc]\n",
    "    # convert to lower case\n",
    "    text = [word.lower() for word in text]\n",
    "    # remove punctuation from each token\n",
    "    text = [w.translate(table) for w in text]\n",
    "    # remove hanging 's' and 'a'\n",
    "    text = [word for word in text if len(word)>1]\n",
    "    # remove tokens with numbers in them\n",
    "    text = [word for word in text if word.isalpha()]\n",
    "    # store as string\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXXKfkiY52_y"
   },
   "outputs": [],
   "source": [
    "data[\"question1\"] = data[\"question1\"].apply(lambda x:clean_question(x))\n",
    "data[\"question2\"] = data[\"question2\"].apply(lambda x:clean_question(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "Wm1x7FfO-nGI",
    "outputId": "e5402cdc-ac5e-426d-b3ee-22e6d7a6b36f"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mkYEK6mDz-6w"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qh20eJmABc_K",
    "outputId": "fc85664b-9be3-4e26-aa83-5ec13aead953"
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer with questions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data[\"question1\"].values+data[\"question2\"].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v7AoYTqeD7y5"
   },
   "outputs": [],
   "source": [
    "# create sequences\n",
    "max_len = 100\n",
    "q1_texts_seq = tokenizer.texts_to_sequences(data[\"question1\"].values)\n",
    "q2_texts_seq = tokenizer.texts_to_sequences(data[\"question2\"].values)\n",
    "\n",
    "q1_texts_seq = pad_sequences(q1_texts_seq,maxlen=max_len)\n",
    "q2_texts_seq = pad_sequences(q2_texts_seq,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbXSABPdJhrh"
   },
   "outputs": [],
   "source": [
    "if os.path.isdir('glove') == False:\n",
    "    os.mkdir('glove')\n",
    "\n",
    "glove_dir = \"datasets/\"\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(glove_dir+'glove.zip', 'r') as z:\n",
    "  z.extractall(\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JDbhWXclJkbP",
    "outputId": "b3614cd5-bbea-4e93-c236-3a74311b2fe1"
   },
   "outputs": [],
   "source": [
    "# Load Glove vectors\n",
    "embeddings_index = {} # empty dictionary\n",
    "f = open(os.path.join(\"glove/\", 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7CUjZjcCJmiq"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 200\n",
    "\n",
    "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in the embedding index will be all zeros\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WfWFaPkLFUu1"
   },
   "outputs": [],
   "source": [
    "X = np.stack((q1_texts_seq, q2_texts_seq), axis=1)\n",
    "y = data[\"is_duplicate\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wrE_9flOGVCU"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_V0HpjaGF296"
   },
   "outputs": [],
   "source": [
    "# Get Question 1/2  train and test features\n",
    "q1_X_train = X_train[:,0]\n",
    "q2_X_train = X_train[:,1]\n",
    "\n",
    "q1_X_test = X_test[:,0]\n",
    "q2_X_test = X_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4-qUDjkv79j"
   },
   "outputs": [],
   "source": [
    "## Define custon metrics\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0 or c2 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + K.epsilon())\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 + K.epsilon())\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    if c2 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + K.epsilon())\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 + K.epsilon())\n",
    "\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpOnjKzTxekr"
   },
   "source": [
    "## Build the model\n",
    "Paraphrase Detection using **Feature Fusion Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzGgSYmZGMaZ"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape,embeddings_dim, embeddings_matrix, vocab_size, max_seq_length, trainable_embeddings, dropout, hidden_units):\n",
    "   \n",
    "    # TODO: Add docstring\n",
    "    X1_input = Input(input_shape, name=\"input_X1\")\n",
    "    X2_input = Input(input_shape, name=\"input_X2\")\n",
    "\n",
    "    # Encoding the inputs using the same weights\n",
    "    # Output shape: (batch_size, max_seq_length, lstm_hidden_units)\n",
    "    embeddor = Embedding(vocab_size, embeddings_dim, weights=[embeddings_matrix], input_length=max_seq_length, trainable=trainable_embeddings)(X1_input)\n",
    "    td = TimeDistributed(Dense(embeddings_dim, activation='relu'))(embeddor)\n",
    "    ld = Lambda(lambda x: K.sum(x, axis=1), output_shape=(embeddings_dim, ))(td)\n",
    "\n",
    "    embeddor1 = Embedding(vocab_size, embeddings_dim, weights=[embeddings_matrix], input_length=max_seq_length, trainable=trainable_embeddings)(X2_input)\n",
    "    td1 = TimeDistributed(Dense(embeddings_dim, activation='relu'))(embeddor1)\n",
    "    ld1 = Lambda(lambda x: K.sum(x, axis=1), output_shape=(embeddings_dim, ))(td1)\n",
    "\n",
    "    cat = concatenate([ld,ld1])\n",
    "    X = Dense(hidden_units, activation=\"relu\")(cat)\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(hidden_units, activation=\"relu\")(X)\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(hidden_units, activation=\"relu\")(X)\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(hidden_units, activation=\"relu\")(X)\n",
    "    X = Dropout(dropout)(X)\n",
    "    X = Dense(1, activation=\"sigmoid\", name=\"output\")(X)\n",
    "\n",
    "    model = Model(inputs=[X1_input, X2_input], outputs=X, name=\"GRN_model\")\n",
    "\n",
    "    optimizer = optimizers.Adam()\n",
    "    # optimizer = optimizers.RMSprop()\n",
    "    model.compile(optimizer=optimizer,\n",
    "                loss=\"binary_crossentropy\",\n",
    "                metrics=['accuracy', precision, recall, f1_score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "colab_type": "code",
    "id": "FOeFGaEQIYTT",
    "outputId": "6a212f85-d538-41e6-a526-0025de87d822"
   },
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "trainable_embeddings = False\n",
    "hidden_units = 200\n",
    "input_shape = (max_len,)\n",
    "model = create_model(input_shape,embedding_dim, embedding_matrix, vocab_size, max_len, trainable_embeddings, dropout, hidden_units)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "wVXEuy6WN23m",
    "outputId": "1f3bc9c9-7fa5-42fa-c70d-2f925e22a57d"
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtDgbPujzLn5"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-ziFUgbYdFb"
   },
   "outputs": [],
   "source": [
    "# Defining a helper function to save the model after each epoch \n",
    "# in which the loss decreases \n",
    "filepath = project_path+'model_paraprase_detection_pad_FFN.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# Defining a helper function to reduce the learning rate each time \n",
    "# the learning plateaus \n",
    "reduce_alpha = ReduceLROnPlateau(monitor ='val_loss', factor = 0.2, patience = 1, min_lr = 0.001)\n",
    "# stop traning if there increase in loss\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n",
    "callbacks = [checkpoint,es,reduce_alpha] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "SpfEXkzIJ9Bt",
    "outputId": "71f1f8a4-296d-4405-de56-363a99217268"
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "history = model.fit(x=[q1_X_train, q2_X_train],\n",
    "                    y=y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=([q1_X_test, q2_X_test], y_test),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3lsvk6k6zVSY"
   },
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Kh50XXqOyeIK",
    "outputId": "fe07988a-4704-4d52-b361-78c7fb5fa2d3"
   },
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(\"All data in history: \", history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "dXCOFKyOyeFH",
    "outputId": "5b7aa95a-155b-45b4-b2db-a6f92c3ba141"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "fig.savefig('model_accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "O3acX2dfyg2i",
    "outputId": "a37d08c0-b7fe-455f-b79b-f4bc2e5a0537"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "fig.savefig('model_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "sxxz13hSyiVp",
    "outputId": "19fdc640-b0de-401d-8c4d-cad6b14cd5f0"
   },
   "outputs": [],
   "source": [
    "# summarize history for precision\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['val_precision'])\n",
    "plt.title('model precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "fig.savefig('model_precision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "dDrxs9RXyjrg",
    "outputId": "c9a2e0a5-9ada-4985-9fe4-c82bd331f49c"
   },
   "outputs": [],
   "source": [
    "# summarize history for recall\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['recall'])\n",
    "plt.plot(history.history['val_recall'])\n",
    "plt.title('model recall')\n",
    "plt.ylabel('recall')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "fig.savefig('model_recall.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ejkwAa4ayk5J",
    "outputId": "14512662-8a27-4f66-e866-ae16ad50c4c3"
   },
   "outputs": [],
   "source": [
    "# summarize history for f1 score\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['f1_score'])\n",
    "plt.plot(history.history['val_f1_score'])\n",
    "plt.title('model f1_score')\n",
    "plt.ylabel('f1_score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "fig.savefig('model_f1_score.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZrOhyXmDzbO3"
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "EUC3fY6MymvB",
    "outputId": "d6f04cf0-da5b-440f-b818-f90613ecb2bc"
   },
   "outputs": [],
   "source": [
    "print('Testing Data Metrics:')\n",
    "loss, accuracy, precision, recall, f1_score = model.evaluate([q1_X_test, q2_X_test], y_test)\n",
    "print('')\n",
    "print('loss      = {0:.4f}'.format(loss))\n",
    "print('accuracy  = {0:.4f}'.format(accuracy))\n",
    "print('precision = {0:.4f}'.format(precision))\n",
    "print('recall    = {0:.4f}'.format(recall))\n",
    "print('F1         = {0:.4f}'.format(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6s3LcE5wpDX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Paraphrase_Detection_FFN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
